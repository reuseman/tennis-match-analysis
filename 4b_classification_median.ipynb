{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Metrics and model evaluation\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import GridSearchCV, learning_curve, validation_curve, cross_validate\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Classifiers\n",
    "import wittgenstein as lw\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Visualization\n",
    "from IPython.display import Image, Markdown, display\n",
    "from sklearn.decomposition import PCA\n",
    "import pydotplus\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "pio.templates.default = \"seaborn\"\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import warnings\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class is used to split the data into train and test and then is used to show a report for each classifier, that contains info about the validation and the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classification:\n",
    "  \n",
    "  # constructors that takes models and data\n",
    "  def __init__(self, X, Y, classes, cv=5):\n",
    "    self._models = []\n",
    "    self.X = X\n",
    "    self.Y = Y\n",
    "    self.classes = classes\n",
    "    # Split the data into training and test sets with a test size of 30%\n",
    "    self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(X, Y, stratify=Y, test_size=0.3, random_state=42)\n",
    "    self._cv = cv\n",
    "\n",
    "  # This function executes a grid search for a specific model and then saves the best model and its parameters\n",
    "  def show_report(self, estimator_name, estimator, parameters, validation_parameter, normalization=True, best_model_position=1):\n",
    "    # add clf to parameters\n",
    "    parameters = {'clf__' + k: v for k, v in parameters.items()}\n",
    "    validation_parameter = 'clf__' + validation_parameter\n",
    "\n",
    "    # Execute grid search for the classifier given the parameters on the training set with 5-fold cross validation\n",
    "    grid_search_clf = self._estimator_to_grid_search(estimator, parameters, cross_validation=self._cv, normalization=normalization)\n",
    "    grid_search_clf.fit(self.x_train, self.y_train)\n",
    "    \n",
    "    # ========================\n",
    "    # Report on VALIDATION SET\n",
    "    # ========================\n",
    "    display(Markdown('#### VALIDATION Report'))\n",
    "    ## Show grid search results with the best top 10 results\n",
    "    display(Markdown(f'##### Top 10 results of the grid search with {self._cv}-fold cross validation'))\n",
    "    self._print_grid_search_results(grid_search_clf)\n",
    "\n",
    "    # If selected rank for the best model is different, pick the best model with the selected rank\n",
    "    if best_model_position != 1:\n",
    "      results = pd.DataFrame(grid_search_clf.cv_results_)[['params', 'mean_train_score', 'mean_test_score', 'rank_test_score']].sort_values(by='rank_test_score')\n",
    "      params = results.iloc[[best_model_position - 1], 0].values[0]\n",
    "      grid_search_clf.best_estimator_.set_params(**params).fit(self.x_train, self.y_train)\n",
    "\n",
    "    # Print Validation metrics for the best model\n",
    "    validation_scores = cross_validate(grid_search_clf.best_estimator_, self.x_train, self.y_train, cv=self._cv, scoring=[\"accuracy\", \"f1\", \"precision\", \"recall\"])\n",
    "    validation_scores = {k: round(np.mean(v), 4) for k, v in validation_scores.items()}\n",
    "    display(Markdown(f'##### Validation metrics for the best model'))\n",
    "    print(\"Accuracy: \" + str(validation_scores['test_accuracy']))\n",
    "    print(\"F1: \" + str(validation_scores['test_f1']))\n",
    "    print(\"Precision: \" + str(validation_scores['test_precision']))\n",
    "    print(\"Recall: \" + str(validation_scores['test_recall']))\n",
    "\n",
    "    # Plot the validation curve\n",
    "    if parameters and validation_parameter:\n",
    "      self._plot_validation_curve(grid_search_clf.best_estimator_, parameters, validation_parameter, cv=self._cv)\n",
    "\n",
    "    # Plot the learning curve\n",
    "    self._plot_learning_curve(grid_search_clf.best_estimator_, cv=self._cv)\n",
    "\n",
    "    # ========================\n",
    "    # Report on TEST SET\n",
    "    # ========================\n",
    "    display(Markdown('#### TEST Report'))\n",
    "    # Test the best classifier on the test set\n",
    "    y_pred = grid_search_clf.predict(self.x_test)\n",
    "\n",
    "    # Get the best classifier of the grid search and add it to the list of best models\n",
    "    self._models.append((estimator_name, grid_search_clf.best_estimator_))\n",
    "\n",
    "    # Show the confusion matrix for the best classifier on the TEST set\n",
    "    self._plot_confusion_matrix(y_pred, labels=grid_search_clf.classes_)\n",
    "\n",
    "    ## Show the metrics for the best classifier on the TEST set\n",
    "    display(Markdown('##### Metrics for the best classifier on the TEST set'))\n",
    "    print(classification_report(self.y_test, y_pred, target_names=self.classes, zero_division=0, sample_weight=None))\n",
    "\n",
    "    # Return the best classifier\n",
    "    return grid_search_clf.best_estimator_\n",
    "\n",
    "  # Return all best models tested with show_report\n",
    "  def get_best_models(self):\n",
    "    return self._models\n",
    "\n",
    "  def _estimator_to_grid_search(self, estimator, parameters, cross_validation=5, normalization=True, random_state=42):\n",
    "    estimator.random_state = random_state\n",
    "    if normalization:\n",
    "        pipeline = Pipeline(steps=[('scaler', MinMaxScaler()), ('clf', estimator)])\n",
    "    else:\n",
    "        pipeline = Pipeline(steps=[('clf', estimator)])\n",
    "\n",
    "    return GridSearchCV(pipeline, param_grid=parameters, cv=cross_validation, n_jobs=-1, refit=True, return_train_score=True)\n",
    "\n",
    "  def _print_grid_search_results(self, grid_search):\n",
    "    df = pd.DataFrame(grid_search.cv_results_)[['params', 'mean_train_score', 'mean_test_score', 'rank_test_score']].sort_values(by='rank_test_score')\n",
    "\n",
    "    # remove the 'clf__' prefix from the parameters column\n",
    "    df['params'] = df['params'].apply(lambda x: {k[5:]: v for k, v in x.items()})\n",
    "    # create a column for each parameter in params column in df_new\n",
    "    df_new = pd.DataFrame()\n",
    "    for param in df['params'].iloc[0].keys():\n",
    "      df_new[param] = df['params'].apply(lambda x: x[param])\n",
    "    # add old columns to new df\n",
    "    df_new = df_new.join(df[['mean_train_score', 'mean_test_score', 'rank_test_score']])\n",
    "\n",
    "    display(df_new.head(10).style.hide_index())\n",
    "\n",
    "  def _plot_confusion_matrix(self, y_pred, labels, normalize=False):\n",
    "    cm = confusion_matrix(self.y_test, y_pred, labels=labels)\n",
    "    if normalize:\n",
    "      cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "      display(Markdown('##### Normalized confusion matrix for the best classifier obtained by the Grid Search on the TEST set'))\n",
    "    else:\n",
    "      display(Markdown('##### Confusion matrix for the best classifier obtained by the Grid Search on the TEST set'))\n",
    "\n",
    "    # px.imshow(cm, x=classes, y=classes, title='Confusion matrix', color_continuous_scale=\"Blues\", labels=dict(x=\"Real value\", y=\"Predicted value\", color=\"Records\"), text_auto=True)\n",
    "    fig = ff.create_annotated_heatmap(cm[[1, 0]], x=self.classes, y=self.classes, colorscale='Blues', showscale=True)\n",
    "    fig.update_layout(xaxis = dict(title='Predicted value'), yaxis = dict(title='Real value'))\n",
    "    fig.show()\n",
    "\n",
    "  def _plot_learning_curve(self, clf, scoring='accuracy', cv=5, train_sizes=np.linspace(.1, 1.0, 5), shuffle=False, random_state=None):\n",
    "\n",
    "    train_sizes, train_scores, test_scores = learning_curve(clf, self.x_train, self.y_train, train_sizes=train_sizes, cv=cv,\n",
    "                                                            scoring=scoring, n_jobs=-1, shuffle=shuffle,\n",
    "                                                            random_state=random_state)\n",
    "    mean_train_score = np.mean(train_scores, axis=1)\n",
    "    std_train_score = np.std(train_scores, axis=1)\n",
    "    mean_test_score = np.mean(test_scores, axis=1)\n",
    "    std_test_score = np.std(test_scores, axis=1)\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=train_sizes, y=mean_train_score, name='train score', line=dict(color='royalblue')))\n",
    "    fig.add_trace(go.Scatter(x=train_sizes, y=mean_train_score + std_train_score, mode=\"lines\", showlegend=False, line=dict(width=0)))\n",
    "    fig.add_trace(go.Scatter(x=train_sizes, y=mean_train_score - std_train_score, fill='tonexty', showlegend=False, \n",
    "      fillcolor='rgba(65,105,225,0.2)',\n",
    "      line_color='rgba(255,255,255,0)',))\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=train_sizes, y=mean_test_score, name='cross-validation score', line=dict(color='firebrick')))\n",
    "    fig.add_trace(go.Scatter(x=train_sizes, y=mean_test_score + std_test_score, mode=\"lines\", showlegend=False, line=dict(width=0)))\n",
    "    fig.add_trace(go.Scatter(x=train_sizes, y=mean_test_score - std_test_score, fill='tonexty', showlegend=False, \n",
    "      fillcolor='rgba(255,107,107,0.2)',\n",
    "      line_color='rgba(255,255,255,0)',))\n",
    "\n",
    "    model_name = str(clf[\"clf\"].__class__.__name__)\n",
    "    fig.update_layout(title=f'Learning Curve for {model_name}',\n",
    "                   xaxis_title='Train set size',\n",
    "                   yaxis_title='Accuracy')\n",
    "    fig.show()\n",
    "\n",
    "  def _plot_validation_curve(self, clf, parameters, validation_parameter, scoring='accuracy', cv=5):\n",
    "    param_range = parameters[validation_parameter]\n",
    "    train_scores, test_scores = validation_curve(clf, self.x_train, self.y_train, param_name=validation_parameter, param_range=param_range,\n",
    "                                                  cv=cv, scoring=scoring, n_jobs=-1)\n",
    "    mean_train_score = np.mean(train_scores, axis=1)\n",
    "    std_train_score = np.std(train_scores, axis=1)\n",
    "    mean_test_score = np.mean(test_scores, axis=1)\n",
    "    std_test_score = np.std(test_scores, axis=1)\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=param_range, y=mean_train_score, name='train score', line=dict(color='royalblue')))\n",
    "    fig.add_trace(go.Scatter(x=param_range, y=mean_train_score + std_train_score, mode=\"lines\", showlegend=False, line=dict(width=0)))\n",
    "    fig.add_trace(go.Scatter(x=param_range, y=mean_train_score - std_train_score, fill='tonexty', showlegend=False, \n",
    "      fillcolor='rgba(65,105,225,0.2)',\n",
    "      line_color='rgba(255,255,255,0)',))\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=param_range, y=mean_test_score, name='cross-validation score', line=dict(color='firebrick')))\n",
    "    fig.add_trace(go.Scatter(x=param_range, y=mean_test_score + std_test_score, mode=\"lines\", showlegend=False, line=dict(width=0)))\n",
    "    fig.add_trace(go.Scatter(x=param_range, y=mean_test_score - std_test_score, fill='tonexty', showlegend=False, \n",
    "      fillcolor='rgba(255,107,107,0.2)',\n",
    "      line_color='rgba(255,255,255,0)',))\n",
    "\n",
    "    param_name = str(validation_parameter).replace(\"clf__\", \"\")\n",
    "    model_name = str(clf[\"clf\"].__class__.__name__)\n",
    "    fig.update_layout(title=f'Validation Curve for {model_name}',\n",
    "                    xaxis_title=param_name,\n",
    "                    yaxis_title='Accuracy')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_players_complete = pd.read_csv(\"./datasets/players_classification.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Median splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_players_complete['is_high_ranked'] = np.digitize(df_players_complete['mean_rank_points'], bins=[df_players_complete['mean_rank_points'].median()])\n",
    "print(df_players_complete['is_high_ranked'].value_counts())\n",
    "px.histogram(df_players_complete, x=\"mean_rank_points\", color=\"is_high_ranked\", title=\"Histogram of mean rank points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_players_complete['is_high_ranked'] = np.digitize(df_players_complete['mean_rank_points'], bins=[df_players_complete['mean_rank_points'].mean()])\n",
    "print(df_players_complete['is_high_ranked'].value_counts())\n",
    "px.histogram(df_players_complete, x=\"mean_rank_points\", color=\"is_high_ranked\", title=\"Histogram of mean rank points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pareto splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowest 80% of mean rank points are considered low ranked\n",
    "df_players_complete['is_high_ranked'] = np.digitize(df_players_complete['mean_rank_points'], bins=[df_players_complete['mean_rank_points'].quantile(0.8)])\n",
    "print(df_players_complete['is_high_ranked'].value_counts())\n",
    "px.histogram(df_players_complete, x=\"mean_rank_points\", color=\"is_high_ranked\", title=\"Histogram of mean rank points with 80% quantile\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choice of the label to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['low_ranked', 'high_ranked']\n",
    "df_players = df_players_complete.copy()\n",
    "df_players['is_high_ranked'] = np.digitize(df_players['mean_rank_points'], bins=[df_players['mean_rank_points'].median()])\n",
    "df_players[['mean_rank_points', 'is_high_ranked']]\n",
    "df_players"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_players_complete.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop correlated features with label\n",
    "features_correlated_with_high_ranked = ['max_rank_points', 'last_rank_points', 'variance_rank_points']\n",
    "\n",
    "# drop all columns with missing values\n",
    "df_players = df_players.dropna(axis=1)\n",
    "features_with_missing_values = ['minutes_entropy']  # this one is computed over minutes that contains a lot of missing values\n",
    "\n",
    "# consider only numeric features\n",
    "df_players = df_players.select_dtypes(include=['int64', 'float64'])\n",
    "\n",
    "features_to_drop = features_correlated_with_high_ranked + features_with_missing_values\n",
    "df_players.drop(columns=features_to_drop, inplace=True)\n",
    "\n",
    "# Drop label\n",
    "df_players.drop(columns=['mean_rank_points'], inplace=True)\n",
    "\n",
    "df_players.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_players.drop(columns=['is_high_ranked'])\n",
    "Y = df_players['is_high_ranked']\n",
    "classification = Classification(X, Y, classes = ['low_ranked', 'high_ranked'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'max_depth': [2,3,4,5,6,7,8,9,10], 'criterion': ['gini', 'entropy'], 'splitter': ['best', 'random'], 'min_samples_split': [3, 5, 7, 9, 12], 'min_samples_leaf': [3, 5, 7, 9, 12], 'max_features': ['sqrt', 'log2', None]}\n",
    "best_model = classification.show_report(\"Decision Tree\", DecisionTreeClassifier(), parameters, validation_parameter='max_depth', normalization=False, best_model_position=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.predict(classification.x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdot_data = tree.export_graphviz(best_model[\"clf\"], out_file=None,\n",
    "                         feature_names=list(classification.x_train.columns),\n",
    "                         class_names=classes,\n",
    "                         filled=True, rounded=True)\n",
    "graph = pydotplus.graph_from_dot_data(cdot_data)\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the names of the most important features according to the model\n",
    "# map the feature importances to the feature names\n",
    "feature_importances = pd.DataFrame({'feature': X.columns, 'importance': best_model[\"clf\"].feature_importances_})\n",
    "# sort according to the importance\n",
    "feature_importances.sort_values('importance', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rule based (RIPPER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\"prune_size\": [0.33, 0.5, 0.8], \"k\": [1, 2]}\n",
    "best_model = classification.show_report(\"Ripper\", lw.RIPPER(), parameters, validation_parameter='prune_size', normalization=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model[\"clf\"].out_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'max_depth': [2,3,4,5,6,7,8,9,10], 'n_estimators': [10, 20, 50, 100], 'max_features': range(1, len(classification.x_train.iloc[0]) + 1), 'bootstrap':[True, False], 'min_samples_leaf': [10, 20, 30, 40, 50], 'min_samples_split': [10, 20, 30, 40, 50]}\n",
    "parameters = {'max_depth': [2,3,4,5,6,7,8,9,10], 'n_estimators': [10, 20, 50, 100], 'max_features': ['sqrt', 'log2', None], 'min_samples_leaf': [10, 20, 30, 40, 50], 'min_samples_split': [10, 20, 30, 40, 50]}\n",
    "best_model = classification.show_report(\"Random Forest\", RandomForestClassifier(), parameters, validation_parameter='max_depth', normalization=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'n_estimators': [10, 20, 50, 100], 'algorithm': ['SAMME', 'SAMME.R'], 'learning_rate': [0.05, 0.1, 0.2, 0.3, 0.5, 0.7, 1.0]}\n",
    "best_model = classification.show_report(\"AdaBoost\", AdaBoostClassifier(), parameters, validation_parameter='n_estimators', normalization=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SQRT heuristic on train set to find the optimal K\n",
    "k_euristic = int(np.sqrt(len(classification.x_train))) + 1\n",
    "# add 20% to k_euristic to get the upper bound\n",
    "k_euristic = int(k_euristic * 1.2)\n",
    "k_range = list(range(1, k_euristic, 2))\n",
    "\n",
    "parameters = {'n_neighbors': k_range, 'weights':['uniform', 'distance'], 'algorithm': ['ball_tree', 'kd_tree', 'brute'], 'metric': ['euclidean', 'manhattan', 'minkowski', 'chebyshev']}\n",
    "best_model = classification.show_report(\"KNN\", KNeighborsClassifier(), parameters, validation_parameter='n_neighbors', normalization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {}\n",
    "best_model = classification.show_report(\"Naive Bayes\", GaussianNB(), parameters, validation_parameter='', normalization=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'C': [0.1, 1, 10, 100, 1000], 'kernel': ['linear', 'poly', 'rbf', 'sigmoid']}\n",
    "best_model = classification.show_report(\"SVM\", SVC(probability=True), parameters, validation_parameter='C', normalization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\"solver\": ['lbfgs', 'sgd', 'adam'], \"alpha\": [0.0001, 0.00001], \"hidden_layer_sizes\": [(10,), (20, )], \"activation\": ['tanh', 'relu'], \"learning_rate\": [ 'invscaling', 'adaptive'], \"max_iter\": [200, 400, 600, 800]}\n",
    "best_model = classification.show_report(\"Neural Network\", MLPClassifier(), parameters, validation_parameter='max_iter', normalization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison (ROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_shape(type=\"line\", x0=0, y0=0, x1=1, y1=1, line=dict(color=\"RoyalBlue\",width=3, dash=\"dash\"))\n",
    "\n",
    "models = classification.get_best_models()\n",
    "\n",
    "for i in range(len(models)):\n",
    "    y_score = models[i][1].predict_proba(classification.x_test)[:, 1]\n",
    "    fpr, tpr, thresholds = roc_curve(classification.y_test, y_score)\n",
    "    auc_score = roc_auc_score(classification.y_test, y_score)\n",
    "    models[i] += (auc_score,)\n",
    "    \n",
    "# Sort according to AUC score\n",
    "models.sort(key=lambda x: x[2], reverse=True)  \n",
    "for model in models:\n",
    "    y_score = model[1].predict_proba(classification.x_test)[:, 1]\n",
    "    fpr, tpr, thresholds = roc_curve(classification.y_test, y_score)\n",
    "    name = f\"{model[0]} - AUC={model[2]:.3f}\"\n",
    "    fig.add_trace(go.Scatter(x=fpr, y=tpr, name=name, mode='lines'))\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title='False Positive Rate',\n",
    "    yaxis_title='True Positive Rate',\n",
    "    yaxis=dict(scaleanchor=\"x\", scaleratio=1),\n",
    "    xaxis=dict(constrain='domain'),\n",
    "    width=700, height=500)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy of all models on train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accuracy_train = pd.DataFrame()\n",
    "df_accuracy_train['Model'] = [model[0] for model in models]\n",
    "df_accuracy_train['Accuracy'] = [model[1].score(classification.x_train, classification.y_train) for model in models]\n",
    "df_accuracy_train['Set'] = ['Train' for model in models]\n",
    "\n",
    "df_accuracy_test = pd.DataFrame()\n",
    "df_accuracy_test['Model'] = [model[0] for model in models]\n",
    "df_accuracy_test['Accuracy'] = [model[1].score(classification.x_test, classification.y_test) for model in models]\n",
    "df_accuracy_test['Set'] = ['Test' for model in models]\n",
    "\n",
    "# merge the dataframes\n",
    "df_accuracy = pd.concat([df_accuracy_train, df_accuracy_test]).reset_index(drop=True)\n",
    "df_accuracy = df_accuracy.sort_values(by=['Set', 'Accuracy', 'Model'], ascending=[1, 0, 1]).reset_index(drop=True)\n",
    "df_accuracy['Accuracy'] = df_accuracy['Accuracy'].round(4)\n",
    "\n",
    "# plotly express barcharth for each model with the train and test accuracy\n",
    "px.bar(df_accuracy, x=\"Model\", y=\"Accuracy\", color=\"Set\", barmode=\"group\", text_auto=True, color_discrete_sequence=[\"firebrick\", \"royalblue\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1, Accuracy, Precision, Recall on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics = pd.DataFrame()\n",
    "df_metrics['Model'] = [model[0] for model in models]\n",
    "\n",
    "df_f1 = pd.DataFrame()\n",
    "df_f1['Model'] = [model[0] for model in models]\n",
    "df_f1['Score'] = [f1_score(classification.y_test, model[1].predict(classification.x_test)) for model in models]\n",
    "df_f1['Metric'] = ['F1' for model in models]\n",
    "df_f1['Score'] = df_f1['Score'].round(4)\n",
    "df_f1 = df_f1.sort_values(by=\"Score\", ascending=False)\n",
    "\n",
    "df_accuracy_test = pd.DataFrame()\n",
    "df_accuracy_test['Model'] = [model[0] for model in models]\n",
    "df_accuracy_test['Score'] = [model[1].score(classification.x_test, classification.y_test) for model in models]\n",
    "df_accuracy_test['Metric'] = ['Accuracy' for model in models]\n",
    "\n",
    "# precision\n",
    "df_precision = pd.DataFrame()\n",
    "df_precision['Model'] = [model[0] for model in models]\n",
    "df_precision['Score'] = [precision_score(classification.y_test, model[1].predict(classification.x_test)) for model in models]\n",
    "df_precision['Metric'] = ['Precision' for model in models]\n",
    "\n",
    "#recall\n",
    "df_recall = pd.DataFrame()\n",
    "df_recall['Model'] = [model[0] for model in models]\n",
    "df_recall['Score'] = [recall_score(classification.y_test, model[1].predict(classification.x_test)) for model in models]\n",
    "df_recall['Metric'] = ['Recall' for model in models]\n",
    "\n",
    "df_metrics = pd.concat([df_f1, df_accuracy_test, df_precision, df_recall]).reset_index(drop=True)\n",
    "df_metrics[\"Score\"] = df_metrics[\"Score\"].round(4)\n",
    "\n",
    "px.bar(df_metrics, x=\"Model\", y=\"Score\", color=\"Metric\", barmode=\"group\", text_auto=True, color_discrete_sequence=[\"firebrick\", \"royalblue\", \"#FFBF00\", \"#32936F\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Visualization of the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for the model with the best AUC score\n",
    "best_model = max(models, key=lambda x: x[2])\n",
    "\n",
    "PCAdata = MinMaxScaler().fit_transform(classification.x_train)\n",
    "X_r = pd.DataFrame(PCA(n_components=2).fit_transform(PCAdata))\n",
    "prediction = best_model[1].predict_proba(classification.x_train)\n",
    "\n",
    "fig = px.scatter(x=X_r[0], y=X_r[1], color=prediction[:, 1], color_continuous_scale='RdBu', symbol=classification.y_train, symbol_map={'0': 'square-dot', '1': 'circle-dot'},  labels={'symbol': 'label', 'color': 'score of <br>first class'})\n",
    "fig.update_traces(marker_size=12, marker_line_width=1.5)\n",
    "fig.update_layout(title=f\"PCA visualization for {best_model[0]}\", legend_orientation='h')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply k-means to identify cluster of good and bad players\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "feautures = ['max_tourney_revenue', 'mean_rank_points', 'lrpOnMxrp', 'matches_won_ratio']\n",
    "df_data = df_players_complete[feautures].reset_index(drop=True)\n",
    "df_data = pd.DataFrame(MinMaxScaler().fit_transform(df_data), columns=df_data.columns)\n",
    "df_data = df_data.round(3)\n",
    "kmeans = KMeans(n_clusters=2, n_init=10, max_iter=100, init=\"k-means++\", random_state=42).fit(df_data)\n",
    "df_players_complete['cluster'] = kmeans.labels_\n",
    "\n",
    "df_players_complete[\"classification\"] = best_model[1].predict(df_players.drop(columns=\"is_high_ranked\"))\n",
    "\n",
    "# Show confusion matrix to see the intersection between classification and cluster\n",
    "cm = confusion_matrix(df_players_complete[\"cluster\"], df_players_complete[\"classification\"])\n",
    "fig = ff.create_annotated_heatmap(cm[[1, 0]], x=[\"Low ranked\",\"High ranked\"], y=[\"Low ranked\", \"High ranked\"], colorscale='Blues', showscale=True)\n",
    "fig.update_layout(xaxis = dict(title='Predicted value'), yaxis = dict(title='K-means'))\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f625ebbd10e36045cf734aa94df07d176492c240bf54797abcd7e809b7fa9e5b"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
